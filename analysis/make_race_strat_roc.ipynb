{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Race stratified ROC curves with 95\\% CI using fast deLong method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.colors\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import join as j_\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pylab\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, classification_report, confusion_matrix\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "\n",
    "cdict = {'white':matplotlib.colors.to_rgb('#66c2a5'), \"asian\":matplotlib.colors.to_rgb('#fc8d62'), 'black':matplotlib.colors.to_rgb('#8da0cb')}\n",
    "\n",
    "def pd_read_pickle_race(path):\n",
    "    results = pd.read_pickle(path)\n",
    "    y_label = np.array(results['labels']).astype(float)\n",
    "    all_races = np.array(results[\"all_races\"])\n",
    "    \n",
    "    results_df = pd.DataFrame({'Y': y_label, \n",
    "                               'p_0': np.array(results['probs'][:,0]),\n",
    "                               'p_1': np.array(results['probs'][:,1]),\n",
    "                               'race': all_races})\n",
    "    results_df.index = results['slide_ids']\n",
    "    results_df.index.name = 'slide_id'\n",
    "    return results_df\n",
    "\n",
    "def get_tpr_fpr_auc(results_df, mean_fpr):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(results_df[\"Y\"], results_df[\"p_1\"], pos_label=1)\n",
    "    auc = metrics.roc_auc_score(results_df[\"Y\"], results_df[\"p_1\"])\n",
    "\n",
    "    interp_tpr = np.interp(mean_fpr, fpr, tpr)\n",
    "    interp_tpr[0] = 0.0\n",
    "\n",
    "    return interp_tpr, auc\n",
    "\n",
    "metric_cols = ['AUC', 'Cutoff', 'Acc',\n",
    "'Y=0 P', 'Y=0 R', 'Y=0 F1', 'Y=0 Support',\n",
    "'Y=1 P', 'Y=1 R', 'Y=1 F1', 'Y=1 Support',\n",
    "'Macro Avg P', 'Macro Avg R', 'Macro Avg F1',\n",
    "'Weight Avg P', 'Weight Avg R', 'Weight Avg F1'\n",
    "]\n",
    "\n",
    "def pd_read_pickle(path):\n",
    "    results = pd.read_pickle(path)\n",
    "    y_label = np.array(results['labels']).astype(float)\n",
    "    \n",
    "    results_df = pd.DataFrame({'Y': y_label, \n",
    "                               'p_0': np.array(results['probs'][:,0]),\n",
    "                               'p_1': np.array(results['probs'][:,1])})\n",
    "    results_df.index = results['slide_ids']\n",
    "    results_df.index.name = 'slide_id'\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def compute_midrank(x):\n",
    "    \"\"\"Computes midranks.\n",
    "    Args:\n",
    "       x - a 1D numpy array\n",
    "    Returns:\n",
    "       array of midranks\n",
    "    \"\"\"\n",
    "    J = np.argsort(x)\n",
    "    Z = x[J]\n",
    "    N = len(x)\n",
    "    T = np.zeros(N, dtype=float)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        j = i\n",
    "        while j < N and Z[j] == Z[i]:\n",
    "            j += 1\n",
    "        T[i:j] = 0.5*(i + j - 1)\n",
    "        i = j\n",
    "    T2 = np.empty(N, dtype=float)\n",
    "    # Note(kazeevn) +1 is due to Python using 0-based indexing\n",
    "    # instead of 1-based in the AUC formula in the paper\n",
    "    T2[J] = T + 1\n",
    "    return T2\n",
    "    \n",
    "\n",
    "def compute_midrank_weight(x, sample_weight):\n",
    "    \"\"\"Computes midranks.\n",
    "    Args:\n",
    "       x - a 1D numpy array\n",
    "    Returns:\n",
    "       array of midranks\n",
    "    \"\"\"\n",
    "    J = np.argsort(x)\n",
    "    Z = x[J]\n",
    "    cumulative_weight = np.cumsum(sample_weight[J])\n",
    "    N = len(x)\n",
    "    T = np.zeros(N, dtype=float)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        j = i\n",
    "        while j < N and Z[j] == Z[i]:\n",
    "            j += 1\n",
    "        T[i:j] = cumulative_weight[i:j].mean()\n",
    "        i = j\n",
    "    T2 = np.empty(N, dtype=float)\n",
    "    T2[J] = T\n",
    "    return T2\n",
    "\n",
    "def fastDeLong_weights(predictions_sorted_transposed, label_1_count, sample_weight):\n",
    "    \"\"\"\n",
    "    The fast version of DeLong's method for computing the covariance of\n",
    "    unadjusted AUC.\n",
    "    Args:\n",
    "       predictions_sorted_transposed: a 2D numpy.array[n_classifiers, n_examples]\n",
    "          sorted such as the examples with label \"1\" are first\n",
    "    Returns:\n",
    "       (AUC value, DeLong covariance)\n",
    "    Reference:\n",
    "     @article{sun2014fast,\n",
    "       title={Fast Implementation of DeLong's Algorithm for\n",
    "              Comparing the Areas Under Correlated Receiver Oerating Characteristic Curves},\n",
    "       author={Xu Sun and Weichao Xu},\n",
    "       journal={IEEE Signal Processing Letters},\n",
    "       volume={21},\n",
    "       number={11},\n",
    "       pages={1389--1393},\n",
    "       year={2014},\n",
    "       publisher={IEEE}\n",
    "     }\n",
    "    \"\"\"\n",
    "    # Short variables are named as they are in the paper\n",
    "    m = label_1_count\n",
    "    n = predictions_sorted_transposed.shape[1] - m\n",
    "    positive_examples = predictions_sorted_transposed[:, :m]\n",
    "    negative_examples = predictions_sorted_transposed[:, m:]\n",
    "    k = predictions_sorted_transposed.shape[0]\n",
    "\n",
    "    tx = np.empty([k, m], dtype=float)\n",
    "    ty = np.empty([k, n], dtype=float)\n",
    "    tz = np.empty([k, m + n], dtype=float)\n",
    "    for r in range(k):\n",
    "        tx[r, :] = compute_midrank_weight(positive_examples[r, :], sample_weight[:m])\n",
    "        ty[r, :] = compute_midrank_weight(negative_examples[r, :], sample_weight[m:])\n",
    "        tz[r, :] = compute_midrank_weight(predictions_sorted_transposed[r, :], sample_weight)\n",
    "    total_positive_weights = sample_weight[:m].sum()\n",
    "    total_negative_weights = sample_weight[m:].sum()\n",
    "    pair_weights = np.dot(sample_weight[:m, np.newaxis], sample_weight[np.newaxis, m:])\n",
    "    total_pair_weights = pair_weights.sum()\n",
    "    aucs = (sample_weight[:m]*(tz[:, :m] - tx)).sum(axis=1) / total_pair_weights\n",
    "    v01 = (tz[:, :m] - tx[:, :]) / total_negative_weights\n",
    "    v10 = 1. - (tz[:, m:] - ty[:, :]) / total_positive_weights\n",
    "    sx = np.cov(v01)\n",
    "    sy = np.cov(v10)\n",
    "    delongcov = sx / m + sy / n\n",
    "    return aucs, delongcov\n",
    "\n",
    "def fastDeLong_no_weights(predictions_sorted_transposed, label_1_count):\n",
    "    \"\"\"\n",
    "    The fast version of DeLong's method for computing the covariance of\n",
    "    unadjusted AUC.\n",
    "    Args:\n",
    "       predictions_sorted_transposed: a 2D numpy.array[n_classifiers, n_examples]\n",
    "          sorted such as the examples with label \"1\" are first\n",
    "    Returns:\n",
    "       (AUC value, DeLong covariance)\n",
    "    Reference:\n",
    "     @article{sun2014fast,\n",
    "       title={Fast Implementation of DeLong's Algorithm for\n",
    "              Comparing the Areas Under Correlated Receiver Oerating\n",
    "              Characteristic Curves},\n",
    "       author={Xu Sun and Weichao Xu},\n",
    "       journal={IEEE Signal Processing Letters},\n",
    "       volume={21},\n",
    "       number={11},\n",
    "       pages={1389--1393},\n",
    "       year={2014},\n",
    "       publisher={IEEE}\n",
    "     }\n",
    "    \"\"\"\n",
    "    # Short variables are named as they are in the paper\n",
    "    m = label_1_count\n",
    "    n = predictions_sorted_transposed.shape[1] - m\n",
    "    positive_examples = predictions_sorted_transposed[:, :m]\n",
    "    negative_examples = predictions_sorted_transposed[:, m:]\n",
    "    k = predictions_sorted_transposed.shape[0]\n",
    "\n",
    "    tx = np.empty([k, m], dtype=float)\n",
    "    ty = np.empty([k, n], dtype=float)\n",
    "    tz = np.empty([k, m + n], dtype=float)\n",
    "    for r in range(k):\n",
    "        tx[r, :] = compute_midrank(positive_examples[r, :])\n",
    "        ty[r, :] = compute_midrank(negative_examples[r, :])\n",
    "        tz[r, :] = compute_midrank(predictions_sorted_transposed[r, :])\n",
    "    aucs = tz[:, :m].sum(axis=1) / m / n - float(m + 1.0) / 2.0 / n\n",
    "    v01 = (tz[:, :m] - tx[:, :]) / n\n",
    "    v10 = 1.0 - (tz[:, m:] - ty[:, :]) / m\n",
    "    sx = np.cov(v01)\n",
    "    sy = np.cov(v10)\n",
    "    delongcov = sx / m + sy / n\n",
    "    return aucs, delongcov\n",
    "\n",
    "def fastDeLong(predictions_sorted_transposed, label_1_count, sample_weight):\n",
    "    if sample_weight is None:\n",
    "        return fastDeLong_no_weights(predictions_sorted_transposed, label_1_count)\n",
    "    else:\n",
    "        return fastDeLong_weights(predictions_sorted_transposed, label_1_count, sample_weight)\n",
    "\n",
    "def compute_ground_truth_statistics(ground_truth, sample_weight):\n",
    "    assert np.array_equal(np.unique(ground_truth), [0, 1])\n",
    "    order = (-ground_truth).argsort()\n",
    "    label_1_count = int(ground_truth.sum())\n",
    "    if sample_weight is None:\n",
    "        ordered_sample_weight = None\n",
    "    else:\n",
    "        ordered_sample_weight = sample_weight[order]\n",
    "\n",
    "    return order, label_1_count, ordered_sample_weight\n",
    "\n",
    "def delong_roc_variance(ground_truth, predictions, sample_weight=None):\n",
    "    \"\"\"\n",
    "    Computes ROC AUC variance for a single set of predictions\n",
    "    Args:\n",
    "       ground_truth: np.array of 0 and 1\n",
    "       predictions: np.array of floats of the probability of being class 1\n",
    "    \"\"\"\n",
    "    order, label_1_count, ordered_sample_weight = compute_ground_truth_statistics(\n",
    "        ground_truth, sample_weight)\n",
    "    predictions = np.asarray(predictions)\n",
    "    predictions_sorted_transposed = predictions[np.newaxis, order]\n",
    "    aucs, delongcov = fastDeLong(predictions_sorted_transposed, label_1_count, ordered_sample_weight)\n",
    "    assert len(aucs) == 1, \"There is a bug in the code, please forward this to the developers\"\n",
    "    return aucs[0], delongcov\n",
    "\n",
    "def get_CI(y_true, y_pred):\n",
    "    alpha = 0.95\n",
    "    auc, auc_cov = delong_roc_variance(y_true, y_pred)\n",
    "    auc_std = np.sqrt(auc_cov)\n",
    "    lower_upper_q = np.abs(np.array([0, 1]) - (1 - alpha) / 2)\n",
    "    ci = stats.norm.ppf(lower_upper_q, loc=auc, scale=auc_std)\n",
    "    ci[ci > 1] = 1\n",
    "    return ci[0], auc, ci[1], auc_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics_binary(y_label, y_prob, cutoff=None, return_pred=False):\n",
    "    \"\"\"\n",
    "    Computing (almost all) binary calculation metrics.\n",
    "    \n",
    "    Args:\n",
    "        - y_label (np.array): (n,)-dim np.array containing ground-truth predictions.\n",
    "        - y_prob (np.array): (n,)-dim np.array containing probability scores (for y=1).\n",
    "        - cutoff (int): Whether to use a Yolan's J cutoff (calculated from a model)\n",
    "        - return_pred (np.array): (n,)-dim np.array containing predictions using Yolan's J.\n",
    "    Return:\n",
    "        - results (list): List of binary classification metrics.\n",
    "    \"\"\"\n",
    "    ### AUC\n",
    "    auc = roc_auc_score(y_label, y_prob)\n",
    "    \n",
    "    ### Yolans J\n",
    "    if cutoff == None:\n",
    "        fpr, tpr, thresholds = roc_curve(y_label, y_prob)\n",
    "        J = tpr - fpr\n",
    "        cutoff = thresholds[np.argmax(J)]\n",
    "    y_pred = np.array(y_prob > cutoff).astype(int)\n",
    "    \n",
    "    ### Classification Report\n",
    "    out = classification_report(y_label, y_pred, output_dict=True, zero_division=0)\n",
    "    if return_pred:\n",
    "        return y_pred, [auc, cutoff, out['accuracy'],\n",
    "            out['0.0']['precision'], out['0.0']['recall'], out['0.0']['f1-score'], out['0.0']['support'],\n",
    "            out['1.0']['precision'], out['1.0']['recall'], out['1.0']['f1-score'], out['1.0']['support'],\n",
    "            out['macro avg']['precision'], out['macro avg']['recall'], out['macro avg']['f1-score'],\n",
    "            out['weighted avg']['precision'], out['weighted avg']['recall'], out['weighted avg']['f1-score'],\n",
    "           ]\n",
    "    else:\n",
    "        return [auc, cutoff, out['accuracy'],\n",
    "                out['0.0']['precision'], out['0.0']['recall'], out['0.0']['f1-score'], out['0.0']['support'],\n",
    "                out['1.0']['precision'], out['1.0']['recall'], out['1.0']['f1-score'], out['1.0']['support'],\n",
    "                out['macro avg']['precision'], out['macro avg']['recall'], out['macro avg']['f1-score'],\n",
    "                out['weighted avg']['precision'], out['weighted avg']['recall'], out['weighted avg']['f1-score'],\n",
    "               ]\n",
    "\n",
    "def find_interesting_fold(eval_path):\n",
    "    metrics = []\n",
    "    y_label_all, y_prob_all = [], []\n",
    "    for i in range(20):\n",
    "        \n",
    "        if os.path.isfile(os.path.join(eval_path, 's_%d_checkpoint_results.pkl' % i)):\n",
    "            results_df = pd_read_pickle(j_(eval_path, 's_%d_checkpoint_results.pkl' % i))\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        y_label = np.array(results_df['Y'])\n",
    "        y_prob = np.array(results_df['p_1'])\n",
    "        y_label_all.append(y_label)\n",
    "        y_prob_all.append(y_prob)\n",
    "        metrics.append(calc_metrics_binary(y_label, y_prob, cutoff=None))\n",
    "\n",
    "\n",
    "    y_label_all = np.hstack(y_label_all)\n",
    "    y_prob_all = np.hstack(y_prob_all)\n",
    "    metrics.append(calc_metrics_binary(y_label_all, y_prob_all, cutoff=None))\n",
    "    metrics = pd.DataFrame(metrics)\n",
    "    metrics.columns = metric_cols\n",
    "    best_fold = metrics['AUC'].argmax()\n",
    "    worst_fold = metrics['AUC'].argmin()\n",
    "\n",
    "    return best_fold, worst_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_results_dir = \"/path/to/tcga/validation/results\"\n",
    "models_to_test_all = os.listdir(root_results_dir)\n",
    "dataroot = 'path/to/test/cohort/results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_test = {\"ABMIL\": [], \"CLAM\": [], \"TMIL\":[]}\n",
    "for model in models_to_test_all:\n",
    "    if \"ABMIL\" in model:\n",
    "        models_to_test[\"ABMIL\"].append(model)\n",
    "    elif \"CLAM\" in model:\n",
    "        models_to_test[\"CLAM\"].append(model)\n",
    "    else:\n",
    "        models_to_test[\"TMIL\"].append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0 \n",
    "for model in models_to_test[\"ABMIL\"]:\n",
    "    \n",
    "    model_name = model.split(\"_\")[2]\n",
    "\n",
    "    eval_path = j_(dataroot, model)\n",
    "\n",
    "    # first find the best and worst folds to plot roc's for \n",
    "    best_fold, worst_fold = find_interesting_fold(eval_path)\n",
    "    folds = {\"best\": best_fold, \"worst\": worst_fold}\n",
    "    \n",
    "    for fold_type in [\"best\"]:\n",
    "\n",
    "        tprs_w = []\n",
    "        aucs_w = []\n",
    "        mean_fpr_w = np.linspace(0, 1, 500)\n",
    "\n",
    "        tprs_b = []\n",
    "        aucs_b = []\n",
    "        mean_fpr_b = np.linspace(0, 1, 500)\n",
    "\n",
    "        tprs_a = []\n",
    "        aucs_a = []\n",
    "        mean_fpr_a = np.linspace(0, 1, 500)\n",
    "\n",
    "        fold_to_eval = folds[fold_type]\n",
    "\n",
    "        # load best fold\n",
    "        results_df = pd_read_pickle_race(j_(eval_path, 's_%d_checkpoint_results.pkl' % fold_to_eval))\n",
    "\n",
    "        # white\n",
    "        tpr_w, auc_w = get_tpr_fpr_auc(results_df[results_df[\"race\"] == 0], mean_fpr_w)\n",
    "        tprs_w.append(tpr_w)\n",
    "        aucs_w.append(auc_w)\n",
    "\n",
    "        # black\n",
    "        tpr_b, auc_b = get_tpr_fpr_auc(results_df[results_df[\"race\"] == 1], mean_fpr_b)\n",
    "        tprs_b.append(tpr_b)\n",
    "        aucs_b.append(auc_b)\n",
    "\n",
    "        # asian\n",
    "        tpr_a, auc_a = get_tpr_fpr_auc(results_df[results_df[\"race\"] == 2], mean_fpr_a)\n",
    "        tprs_a.append(tpr_a)\n",
    "        aucs_a.append(auc_a)\n",
    "        \n",
    "        # get overall mean AUC and CI \n",
    "        low_auc, mean_auc, high_auc, std = get_CI(results_df[\"Y\"], results_df[\"p_1\"])\n",
    "        print(\"{} = {:.3f} ({:.3f}, {:.3f}) {:.3f}\".format(model, mean_auc, low_auc, high_auc, std))\n",
    "        \n",
    "        # white \n",
    "        mean_tpr_w = np.mean(tprs_w, axis=0)\n",
    "        mean_tpr_w[-1] = 1.0\n",
    "        mean_auc_w = np.mean(aucs_w)\n",
    "\n",
    "        df_sub = results_df[results_df[\"race\"] == 0]\n",
    "        y_label, y_pred = df_sub[\"Y\"], df_sub[\"p_1\"]\n",
    "\n",
    "        low_auc_w, mean_auc_w, high_auc_w, std = get_CI(y_label, y_pred)\n",
    "        print(\"mean white auc = {:.3f} +/- {:.3f}\".format(mean_auc_w, std))\n",
    "\n",
    "        ci_w = high_auc_w - mean_auc_w\n",
    "        tprs_upper_w = np.minimum(mean_tpr_w+ci_w, 1)\n",
    "        tprs_lower_w = np.maximum(mean_tpr_w-ci_w, 0)\n",
    "\n",
    "        label_w = r'%s (%0.3f $\\pm$ %0.3f)' % (\"White\", mean_auc_w, ci_w)\n",
    "\n",
    "        # black\n",
    "        mean_tpr_b = np.mean(tprs_b, axis=0)\n",
    "        mean_tpr_b[-1] = 1.0\n",
    "        mean_auc_b = np.mean(aucs_b)\n",
    "\n",
    "        df_sub = results_df[results_df[\"race\"] == 1]\n",
    "        y_label, y_pred = df_sub[\"Y\"], df_sub[\"p_1\"]\n",
    "        low_auc_b, mean_auc_b, high_auc_b, std = get_CI(y_label, y_pred)\n",
    "        print(\"mean black auc = {:.3f} +/- {:.3f}\".format(mean_auc_b, std))\n",
    "\n",
    "        ci_b = high_auc_b - mean_auc_b\n",
    "        tprs_upper_b = np.minimum(mean_tpr_b+ci_b, 1)\n",
    "        tprs_lower_b = np.maximum(mean_tpr_b-ci_b, 0)\n",
    "\n",
    "        label_b = r'%s (%0.3f $\\pm$ %0.3f)' % (\"Black\", mean_auc_b, ci_b)\n",
    "\n",
    "        # asian\n",
    "        mean_tpr_a = np.mean(tprs_a, axis=0)\n",
    "        mean_tpr_a[-1] = 1.0\n",
    "        mean_auc_a = np.mean(aucs_a)\n",
    "\n",
    "        df_sub = results_df[results_df[\"race\"] == 2]\n",
    "        y_label, y_pred = df_sub[\"Y\"], df_sub[\"p_1\"]\n",
    "        low_auc_a, mean_auc_a, high_auc_a, std = get_CI(y_label, y_pred)\n",
    "        print(\"mean asian auc = {:.3f} +/- {:.3f}\".format(mean_auc_a, std))\n",
    "\n",
    "        ci_a = high_auc_a - mean_auc_a\n",
    "        tprs_upper_a = np.minimum(mean_tpr_a+ci_a, 1)\n",
    "        tprs_lower_a = np.maximum(mean_tpr_a-ci_a, 0)\n",
    "\n",
    "        label_a = r'%s (%0.3f $\\pm$ %0.3f)' % (\"Asian\", mean_auc_a, ci_a)\n",
    "\n",
    "        fi = pylab.figure(figsize=(10,10), dpi=600, linewidth=0.2)\n",
    "        fig, ax = plt.subplots(figsize=(10,10))\n",
    "        mpl.rcParams['axes.linewidth'] = 3 #set the value globally\n",
    "        plt.rcParams.update({'font.size':22})\n",
    "\n",
    "        ax.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"k\", alpha=0.8)\n",
    "\n",
    "        # white\n",
    "        plt.plot(mean_fpr_w, mean_tpr_w, color=cdict[\"white\"], label=label_w, lw=4, alpha=0.8)\n",
    "        plt.fill_between(mean_fpr_w, tprs_lower_w, tprs_upper_w, color=cdict[\"white\"], alpha=0.1)\n",
    "\n",
    "        # black\n",
    "        plt.plot(mean_fpr_b, mean_tpr_b, color=cdict[\"black\"], label=label_b, lw=4, alpha=0.8)\n",
    "        plt.fill_between(mean_fpr_b, tprs_lower_b, tprs_upper_b, color=cdict[\"black\"], alpha=0.1)\n",
    "\n",
    "        # asian\n",
    "        plt.plot(mean_fpr_a, mean_tpr_a, color=cdict[\"asian\"], label=label_a, lw=4, alpha=0.8)\n",
    "        plt.fill_between(mean_fpr_a, tprs_lower_a, tprs_upper_a, color=cdict[\"asian\"], alpha=0.1)\n",
    "\n",
    "        ax.set_xlabel('1 - Specificity')\n",
    "        ax.set_ylabel('Sensitivity')\n",
    "        ax.set_xlim([0.,1.0])\n",
    "        ax.set_ylim([0.,1.0])\n",
    "        \n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "        ax.set_xticks(np.arange(0, 1.001, 0.2))\n",
    "        ax.set_yticks(np.arange(0, 1.001, 0.2))\n",
    "        \n",
    "        ax.tick_params(        \n",
    "            which='both',      \n",
    "            bottom=False,      \n",
    "            top=False,\n",
    "            left=False,\n",
    "            right=False,         \n",
    "            labelbottom=False,\n",
    "            labelleft=False) \n",
    "\n",
    "\n",
    "        plt.xlim([-0.05, 1.05])\n",
    "        plt.ylim([-0.05, 1.05])\n",
    "        ax.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        ax.legend(loc=\"lower right\")\n",
    "\n",
    "        print(\"Done with {} ({})\".format(model, fold_type)) \n",
    "        count += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('damm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "317979855715bdc64e281e6cc6b025cca10ad5b55e2bd83e4eb38ef59544576f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
